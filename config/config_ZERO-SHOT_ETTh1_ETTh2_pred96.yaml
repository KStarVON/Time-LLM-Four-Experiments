# Basic config
task_name: long_term_forecast # task name, options: [long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]
is_training: 1 # status
model_id: ETTh1_ETTh2_512_96 # model id
model_comment: TimeLLM-ETTh1_ETTh2 # prefix when saving test results
model: TimeLLM # model name, options: [Autoformer, DLinear]
seed: 2021 # random seed

# Data loader
data_pretrain: ETTh1 # dataset type
data: ETTh2 # dataset type
root_path: ./dataset/ETT-small # root path of the data file
data_path: ETTh2.csv # data file
data_path_pretrain: ETTh1.csv # data file
features: M # forecasting task, options: [M, S, MS]; M: multivariate predict multivariate, S: univariate predict univariate, MS: multivariate predict univariate
target: OT # target feature in S or MS task
loader: modal # dataset type
freq: h # freq for time features encoding, options: [s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly]; you can also use more detailed freq like 15min or 3h
checkpoints: ./checkpoints/ # location of model checkpoints

# Forecasting task
seq_len: 512 # input sequence length
label_len: 48 # start token length
pred_len: 96 # prediction sequence length
seasonal_patterns: Monthly # subset for M4

# Model define
enc_in: 7 # encoder input size
dec_in: 7 # decoder input size
c_out: 7 # output size
d_model: 32 # dimension of model
n_heads: 8 # num of heads
e_layers: 2 # num of encoder layers
d_layers: 1 # num of decoder layers
d_ff: 128 # dimension of fully connected network (FCN)
moving_avg: 25 # window size of moving average
factor: 3 # attention factor
dropout: 0.1 # dropout rate
embed: timeF # time features encoding, options: [timeF, fixed, learned]
activation: gelu # activation function
output_attention: false # whether to output attention in encoder
patch_len: 16 # patch length
stride: 8 # stride
prompt_domain: 0 # prompt domain
llm_model: GPT2 # LLM model options: [LLAMA, GPT2, BERT]
llm_dim: 768 # LLM model dimension; LLama7b: 4096, GPT2-small: 768, BERT-base: 768

# Optimization
num_workers: 0 # data loader num workers
itr: 1 # experiments times
train_epochs: 5 # train epochs
align_epochs: 10 # alignment epochs
batch_size: 24 # batch size of train input data
eval_batch_size: 8 # batch size of model evaluation
patience: 5 # early stopping patience
learning_rate: 0.0001 # optimizer learning rate
des: test # exp description
loss: MSE # loss function
lradj: type1 # adjust learning rate
pct_start: 0.2 # percentage of start
use_amp: false # use automatic mixed precision training
llm_layers: 6 # number of layers in LLM
percent: 100 # percentage
